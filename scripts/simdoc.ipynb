{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matchzoo as mz\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rank_bm25 import BM25Okapi\n",
    "from multiprocessing import Pool,Lock\n",
    "import tqdm\n",
    "import catboost\n",
    "import operator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lg\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data \n",
    "queries = {}\n",
    "with open('./norm_q.txt') as fin:\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        line = line[:-1].split('\\t')\n",
    "        if line[0] == '':\n",
    "            line.pop(0)\n",
    "        queries[line[0]] = line[1]\n",
    "\n",
    "        titles= {}\n",
    "with open(\"./norm_tits.txt\" ,'r', encoding='utf-8') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line.split('\\t')\n",
    "        titles[line[0]]= line[1][:-1]\n",
    "\n",
    "train=[]\n",
    "train_titles = []\n",
    "train_queries = []\n",
    "trq = set()\n",
    "with open(\"./train.marks.tsv/train.marks.tsv\", 'r') as fin:\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split('\\t')\n",
    "        if line[1] in titles.keys():\n",
    "            train.append([line[0],line[1], line[2]])\n",
    "            train_titles.append([line[1], titles[line[1]]])\n",
    "            if line[0] not in trq:\n",
    "                train_queries.append([line[0], queries[line[0]]])\n",
    "                trq.add(line[0])\n",
    "\n",
    "                \n",
    "test=[]\n",
    "test_titles = []\n",
    "test_queries = []\n",
    "teq = set()\n",
    "with open(\"./sample.csv/sample.csv\", 'r') as fin:\n",
    "    fin.readline()\n",
    "    for line in fin.readlines():\n",
    "        line=line[:-1].split(',')\n",
    "        if line[1] in titles.keys():\n",
    "            test.append([line[0],line[1],-1])\n",
    "            test_titles.append([line[1], titles[line[1]]])\n",
    "            if line[0] not in teq:\n",
    "                test_queries.append([line[0], queries[line[0]]])\n",
    "                teq.add(line[0])\n",
    "\n",
    "alq = list(train) + list(test)\n",
    "dctqs = {}\n",
    "for q, t, n in alq:\n",
    "    if q not in dctqs.keys():\n",
    "        dctqs[q] = []\n",
    "    dctqs[q].append(t)\n",
    "alqueries = train_queries + test_queries\n",
    "id_q = {}\n",
    "for el in alqueries:\n",
    "    id_q[el[0]] = el[1]   \n",
    "\n",
    "altits = {}\n",
    "for el in train_titles + test_titles:\n",
    "    altits[el[0]] = el[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large USE\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "import sentencepiece\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3\")\n",
    "\n",
    "def cos(sp1,sp2):\n",
    "    #sp1 = sp1.todense()\n",
    "    #sp2 = sp2.todense()\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))\n",
    "\n",
    "import time\n",
    "def getsimuse(qid):\n",
    "    raw_tits = {}\n",
    "    global id_q\n",
    "    global altits\n",
    "    global embed\n",
    "    #t1 = time.time()\n",
    "    for dc in dctqs[qid]:\n",
    "        try:\n",
    "            raw_tits[dc] = altits[dc]\n",
    "        except:\n",
    "            print(qid, dc)\n",
    "    q = embed([id_q[qid]]).numpy()\n",
    "    embs = embed(list(raw_tits.values())).numpy()\n",
    "    #t2 = time.time()\n",
    "    #print(t2 - t1)\n",
    "    results = []\n",
    "    with open('./luse/{}.txt'.format(qid),'w') as fout:\n",
    "        for i,doc in enumerate(raw_tits.keys()):\n",
    "            fout.write(str(qid) + '\\t' +str(doc) + '\\t' + str(cos(q,embs[i])) + '\\n')\n",
    "    #print(time.time() - t2)\n",
    "    del embs\n",
    "    del q\n",
    "    del raw_tits\n",
    "    return results\n",
    "for q in tqdm.tqdm(dctqs.keys()):\n",
    "    r = getsimuse(q)\n",
    "lfiles = os.listdir(outdata)\n",
    "with open('luse.txt','w') as fout:\n",
    "    for file in lfiles:\n",
    "        with open(outdata+file) as fin:\n",
    "            line = fin.readline()\n",
    "            while line != '':\n",
    "                fout.write(line)\n",
    "                line = fin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtitles = [TaggedDocument(doc, [i]) for i, doc in train_titles]\n",
    "dqueries = [TaggedDocument(doc, [i]) for i, doc in train_queries]\n",
    "dstitles = [TaggedDocument(doc, [i]) for i, doc in test_titles]\n",
    "dsqueries = [TaggedDocument(doc, [i]) for i, doc in test_queries]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docvec similarity on queries\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aq = [TaggedDocument(doc,[i]) for i, doc in alqueries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = Doc2Vec(aq, vector_size=128, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pe/.conda/envs/pycode/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "alpha = 0.0025\n",
    "dv = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0)\n",
    "dv.build_vocab(aq)\n",
    "for epoch in range(max_epochs):\n",
    "    dv.train(aq,\n",
    "    total_examples=dv.corpus_count,\n",
    "    epochs=dv.iter)\n",
    "    # decrease the learning rate\n",
    "    dv.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    dv.min_alpha = dv.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvecs = {}\n",
    "for el in alqueries:\n",
    "    dvecs[el[0]] = dv.docvecs[el[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(sp1,sp2):\n",
    "    #sp1 = sp1.todense()\n",
    "    #sp2 = sp2.todense()\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "simdoc = {}\n",
    "def getsim(key):\n",
    "    global dvecs\n",
    "    tmp = {}\n",
    "    for tkey in dvecs.keys():\n",
    "        if tkey != key:\n",
    "            tmp[tkey] = cos(dvecs[key],dvecs[tkey])\n",
    "    sortres = sorted(tmp.items(), key = operator.itemgetter(1), reverse = True)\n",
    "    simdoc[key] = []\n",
    "    res = []\n",
    "    for i in range(20):\n",
    "        res.append(sortres[i][0])\n",
    "    return [key,res]\n",
    "p = Pool(4)\n",
    "sims = p.map(getsim, [key for key in dvecs.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('oldsimdoc.txt','w') as fout:\n",
    "    for res in sims:\n",
    "        fout.write(res[0]+'\\t')\n",
    "        for el in res[1]:\n",
    "            fout.write(el+' ')\n",
    "        fout.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qa USE\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tensorflow_text\n",
    "module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3')\n",
    "outdata = './quse/'\n",
    "try:\n",
    "    os.mkdir(outdata)\n",
    "except:\n",
    "    n = None\n",
    "def getsimqa(qid):\n",
    "    global id_q\n",
    "    global altits\n",
    "    global module\n",
    "    global dctqs\n",
    "    qe = module.signatures['question_encoder'](tf.constant(id_q))['outputs'].numpy()\n",
    "    with open(outdata+qid+'.txt','w') as fout:\n",
    "        for doc in dctqs.keys():\n",
    "            try:\n",
    "                title = altits[doc]\n",
    "                with open('./data/{}.txt'.format(doc)) as fin:\n",
    "                    text = fin.readline()\n",
    "                re = module.signatures['response_embeddings'](input = tf.constant(title), context = tf.constant(text))['outputs'].numpy()\n",
    "                fout.write(qid+'\\t' + doc + '\\t' + str(cos(qe,re)) + '\\n')\n",
    "                del title\n",
    "                del text\n",
    "                del re\n",
    "            except:\n",
    "                continue\n",
    "for q in tqdm.tqdm(list(dctqs.keys())):\n",
    "    getsimqa(q)\n",
    "qfiles = os.listdir(outdata)\n",
    "with open('quse.txt','w') as fout:\n",
    "    for file in qfiles:\n",
    "        with open(outdata+file) as fin:\n",
    "            line = fin.readline()\n",
    "            while line != '':\n",
    "                fout.write(line)\n",
    "                line = fin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiliggual USE\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from absl import logging\n",
    "import sentencepiece\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "\n",
    "def cos(sp1,sp2):\n",
    "    #sp1 = sp1.todense()\n",
    "    #sp2 = sp2.todense()\n",
    "    return np.float64(np.dot(sp1,sp2.T)/np.linalg.norm(sp1)/np.linalg.norm(sp2))\n",
    "\n",
    "import time\n",
    "def getsimuse(qid):\n",
    "    raw_tits = {}\n",
    "    global id_q\n",
    "    global altits\n",
    "    global embed\n",
    "    #t1 = time.time()\n",
    "    for dc in dctqs[qid]:\n",
    "        try:\n",
    "            raw_tits[dc] = altits[dc]\n",
    "        except:\n",
    "            print(qid, dc)\n",
    "    q = embed([id_q[qid]]).numpy()\n",
    "    embs = embed(list(raw_tits.values())).numpy()\n",
    "    #t2 = time.time()\n",
    "    #print(t2 - t1)\n",
    "    results = []\n",
    "    with open('./muse/{}.txt'.format(qid),'w') as fout:\n",
    "        for i,doc in enumerate(raw_tits.keys()):\n",
    "            fout.write(str(qid) + '\\t' +str(doc) + '\\t' + str(cos(q,embs[i])) + '\\n')\n",
    "    #print(time.time() - t2)\n",
    "    del embs\n",
    "    del q\n",
    "    del raw_tits\n",
    "    return results\n",
    "for q in tqdm.tqdm(dctqs.keys()):\n",
    "    r = getsimuse(q)\n",
    "mfiles = os.listdir('./muse/')\n",
    "with open('muse.txt','w') as fout:\n",
    "    for file in mfiles:\n",
    "        with open('./muse/muse/'+file) as fin:\n",
    "            line = fin.readline()\n",
    "            while line != '':\n",
    "                fout.write(line)\n",
    "                line = fin.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docvec similarity on titles\n",
    "from multiprocessing import Pool\n",
    "def docdst(key):\n",
    "    lst = []\n",
    "    global dctqs\n",
    "    global altits\n",
    "    global id_q\n",
    "    for el in dctqs[key]:\n",
    "        lst.append([el,altits[el]])\n",
    "    td = [TaggedDocument(doc,[i]) for i, doc in lst]\n",
    "    td.append(TaggedDocument(id_q[key],[1111111]))\n",
    "    dv = Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=2, sample = 0)\n",
    "    dv.build_vocab(td)\n",
    "    for epoch in range(max_epochs):\n",
    "        dv.train(tagged_data,\n",
    "        total_examples=dv.corpus_count,\n",
    "        epochs=dv.iter)\n",
    "        dv.alpha -= 0.0002\n",
    "        dv.min_alpha = dv.alpha\n",
    "    with open('./dv/{}.txt'.format(key),'w') as fout:\n",
    "        for el in dctqs[key]:\n",
    "            fout.write(key + '\\t' + el + '\\t' + str(cos(dv.docvecs[1111111],dv.docvecs[el]))+'\\n')\n",
    "    return key\n",
    "p = Pool(6)\n",
    "p.map(docdst,[key for key in dctqs.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext trained\n",
    "import fasttext.util\n",
    "import fasttext\n",
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "model = FastText(min_count=1, size = 10, workers=8, negative=5, iter=10, min_n=2, max_n=10)\n",
    "def getsim(q):\n",
    "    global dctqs\n",
    "    global id_q\n",
    "    global altits\n",
    "    raw = {}\n",
    "    model = FastText(min_count=1, size = 10, workers=2, negative=3, iter=10, min_n=2, max_n=9)\n",
    "    for doc in dctqs[q]:\n",
    "        try:\n",
    "            raw[doc] = word_tokenize(altits[doc])\n",
    "        except:\n",
    "            continue\n",
    "    raw['-1'] = word_tokenize(id_q[q])\n",
    "    model.build_vocab(raw.values())\n",
    "    model.train(raw.values(), total_examples=model.corpus_count, epochs=10)\n",
    "    with open('./trft/{}.txt'.format(q),'w') as fout:\n",
    "        for outdoc in raw.keys():\n",
    "            if outdoc != '-1':\n",
    "                fout.write(str(outdoc)+ '\\t'+str(cos(model.wv[altits[outdoc].replace(' ','_')], model.vw[id_q[q]].replace(' ','_'))) + '\\n')\n",
    "    return\n",
    "p = Pool(4)\n",
    "sims = p.map(getsim, [key for key in dctqs.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext wikipedia\n",
    "fasttext.util.download_model('ru', if_exists='ignore')\n",
    "ft = fasttext.load_model('cc.ru.300.bin')\n",
    "def getsim(q):\n",
    "    global dctqs\n",
    "    global id_q\n",
    "    global altits\n",
    "    global ft\n",
    "    raw = {}\n",
    "    for doc in dctqs[q]:\n",
    "        try:\n",
    "            raw[doc] = ft.get_word_vector(altits[doc])\n",
    "        except:\n",
    "            continue\n",
    "    raw['-1'] = ft.get_word_vector(id_q[q])\n",
    "    with open('./ftsim/{}.txt'.format(q),'w') as fout:\n",
    "        for outdoc in raw.keys():\n",
    "            if outdoc != '-1':\n",
    "                fout.write(str(outdoc)+ '\\t'+str(cos(raw[outdoc],raw['-1'])) + '\\n')\n",
    "    return\n",
    "p = Pool(4)\n",
    "sims = p.map(getsim, [key for key in dctqs.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
